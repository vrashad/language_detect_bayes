{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0aa6cd-6685-451c-b6d9-de2da3673f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas==2.2.3 scikit-learn==1.6.1 matplotlib==3.10.1 joblib==1.4.2 seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2edee2-dd0a-4825-ae4a-815cd83a2621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded dataset with 2140589 records\n",
      "After cleaning 2140588 records remain\n",
      "\n",
      "Language distribution in dataset:\n",
      "ru: 1528909 words (71.4%)\n",
      "en: 463533 words (21.7%)\n",
      "az: 148146 words (6.9%)\n",
      "\n",
      "Data split into training (1712470 words) and test (428118 words) sets\n",
      "\n",
      "Training improved model...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "class ImprovedLanguageClassifier:\n",
    "    def __init__(self, \n",
    "                 ngram_range=(1, 4),\n",
    "                 alpha=0.05,\n",
    "                 max_features=10000,\n",
    "                 use_tfidf=True,\n",
    "                 model_type='naive_bayes',\n",
    "                 class_prior=None):\n",
    "        \n",
    "        if use_tfidf:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                analyzer='char',\n",
    "                ngram_range=ngram_range,\n",
    "                lowercase=True,\n",
    "                max_features=max_features,\n",
    "                sublinear_tf=True\n",
    "            )\n",
    "        else:\n",
    "            self.vectorizer = CountVectorizer(\n",
    "                analyzer='char',\n",
    "                ngram_range=ngram_range,\n",
    "                lowercase=True,\n",
    "                max_features=max_features\n",
    "            )\n",
    "        \n",
    "        if model_type == 'svm':\n",
    "            self.classifier = LinearSVC(C=1.0, class_weight='balanced')\n",
    "        else:\n",
    "            self.classifier = MultinomialNB(alpha=alpha, class_prior=class_prior)\n",
    "        \n",
    "        self.model = Pipeline([\n",
    "            ('vectorizer', self.vectorizer),\n",
    "            ('classifier', self.classifier)\n",
    "        ])\n",
    "        \n",
    "        self.languages = None\n",
    "        self.model_type = model_type\n",
    "        self.class_prior = class_prior\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.class_prior is None and self.model_type == 'naive_bayes':\n",
    "            class_counts = pd.Series(y).value_counts()\n",
    "            total = len(y)\n",
    "            class_weights = {cls: total / count for cls, count in class_counts.items()}\n",
    "            sum_weights = sum(class_weights.values())\n",
    "            self.class_prior = np.array([class_weights[cls] / sum_weights for cls in sorted(class_weights.keys())])\n",
    "            \n",
    "            if self.model_type == 'naive_bayes':\n",
    "                self.model.named_steps['classifier'] = MultinomialNB(\n",
    "                    alpha=self.model.named_steps['classifier'].alpha,\n",
    "                    class_prior=self.class_prior\n",
    "                )\n",
    "        \n",
    "        self.model.fit(X, y)\n",
    "        self.languages = list(sorted(set(y)))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.model_type == 'naive_bayes':\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            decision_values = self.model.decision_function(X)\n",
    "            if decision_values.ndim == 1:\n",
    "                decision_values = np.column_stack([-decision_values, decision_values])\n",
    "            \n",
    "            exp_scores = np.exp(decision_values - np.max(decision_values, axis=1, keepdims=True))\n",
    "            proba = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "            return proba\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'report': report,\n",
    "            'confusion_matrix': cm,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        joblib.dump(self, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        return joblib.load(filepath)\n",
    "    \n",
    "    def get_model_size(self):\n",
    "        return len(self.model.named_steps['vectorizer'].get_feature_names_out())\n",
    "\n",
    "\n",
    "def find_optimal_parameters(X_train, y_train, X_test, y_test):\n",
    "    print(\"Finding optimal parameters...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'vectorizer__ngram_range': [(1, 3), (1, 4), (1, 5)],\n",
    "        'vectorizer__max_features': [5000, 10000],\n",
    "        'classifier__alpha': [0.001, 0.01, 0.05, 0.1, 0.5]\n",
    "    }\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(analyzer='char', lowercase=True, sublinear_tf=True)),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='accuracy', \n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_accuracy = best_model.score(X_test, y_test)\n",
    "    print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return grid_search.best_params_, best_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Loading dataset...\")\n",
    "        data = pd.read_csv('cleaned_dataset_with_translit_error.csv')\n",
    "        print(f\"Loaded dataset with {len(data)} records\")\n",
    "        \n",
    "        data = data.dropna()\n",
    "        data = data[data['word'].str.strip() != '']\n",
    "        data = data.drop_duplicates()\n",
    "        print(f\"After cleaning {len(data)} records remain\")\n",
    "        \n",
    "        print(\"\\nLanguage distribution in dataset:\")\n",
    "        lang_distribution = data['language'].value_counts()\n",
    "        for lang, count in lang_distribution.items():\n",
    "            print(f\"{lang}: {count} words ({count/len(data)*100:.1f}%)\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data['word'], data['language'], \n",
    "            test_size=0.2, random_state=42, stratify=data['language']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nData split into training ({len(X_train)} words) and test ({len(X_test)} words) sets\")\n",
    "        \n",
    "        print(\"\\nTraining improved model...\")\n",
    "        improved_classifier = ImprovedLanguageClassifier(\n",
    "            ngram_range=(1, 4),\n",
    "            alpha=0.05,\n",
    "            max_features=10000,\n",
    "            use_tfidf=True\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        improved_classifier.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        model_path = \"models/improved_language_classifier.joblib\"\n",
    "        improved_classifier.save_model(model_path)\n",
    "        \n",
    "        print(\"\\nEvaluating improved model on test set:\")\n",
    "        evaluation = improved_classifier.evaluate(X_test, y_test)\n",
    "        \n",
    "        print(f\"Accuracy: {evaluation['accuracy']:.4f}\")\n",
    "        print(\"\\nClassification report:\")\n",
    "        print(evaluation['report'])\n",
    "        \n",
    "        print(\"\\nTesting on problematic words:\")\n",
    "        problem_words = [\"kişi\", \"qadın\", \"koynek\", \"hello\", \"привет\"]\n",
    "        \n",
    "        for word in problem_words:\n",
    "            lang = improved_classifier.predict([word])[0]\n",
    "            probas = improved_classifier.predict_proba([word])[0]\n",
    "            top_prob = max(probas) * 100\n",
    "            print(f\"Word: '{word}' -> Language: {lang} (confidence: {top_prob:.2f}%)\")\n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772224c7-3fdd-4acb-bcb4-c728a2c168b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
